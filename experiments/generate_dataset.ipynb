{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils import DATA_ROOT, load_json, load_jsonl_iteratively, load_config\n",
    "\n",
    "NUM_1B = 1e+9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"test_config\")\n",
    "data_root = config['dataset']['dataset-dir']\n",
    "save_root = config['dataset']['exp-dir']\n",
    "collections = config['dataset']['collection']\n",
    "btoken_kg = collections['knowledge']\n",
    "btoken_ct = collections['crosslingual-transfer']['en-ja']\n",
    "save_dir = os.path.join(config['dataset']['exp-dir'], config['dataset']['name'])\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "tokenizer_type = config['dataset']['tokenizer']['tokenizer-type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def write_text(src, tgt, b_tokens, tokenizer_type, lang, record_ids=False, docids=None):\n",
    "    all_cnts = 0\n",
    "    if record_ids:\n",
    "        assert docids is None, \"docids shouldn't be provided if record_ids is True\"\n",
    "        docid_fn = Path(tgt).parent / \"doc_ids.jsonl\"\n",
    "        docid_fp = open(docid_fn, 'a', encoding=\"utf8\") if record_ids else None\n",
    "        print(f\"Writing doc ids to {docid_fn}\")\n",
    "    \n",
    "    if docids is not None:\n",
    "        assert isinstance(docids, set), \"docids should be a set\"\n",
    "        assert record_ids is False, \"docids shouldn't be provided if record_ids is True\"\n",
    "        \n",
    "\n",
    "    with open(tgt, 'a', encoding=\"utf8\") as fp:\n",
    "        num_tokens = b_tokens * NUM_1B\n",
    "        while all_cnts < num_tokens:\n",
    "            desc = f\"{src} → {tgt} ({num_tokens:.2e} tokens)\"\n",
    "            for item in tqdm(load_jsonl_iteratively(src), desc=desc):\n",
    "                if f'{tokenizer_type}' not in item:\n",
    "                    raise NotImplementedError(f\"Please run `python3 tokenization` first to get tokens for each data item for file {src}, with tokenizer {tokenizer_type}\")\n",
    "                if record_ids:\n",
    "                    docid_fp.write(f\"{json.dumps({'docid': item['docid'], 'lang': lang}, ensure_ascii=False)}\\n\")\n",
    "                if docids is not None:\n",
    "                    if item['docid'] not in docids:\n",
    "                        continue\n",
    "\n",
    "                string = json.dumps({'text': item['text']}, ensure_ascii=False)\n",
    "                fp.write(f\"{string}\\n\")\n",
    "                \n",
    "                cnt = item[f'{tokenizer_type}']['num_tokens'] \n",
    "                all_cnts += cnt\n",
    "                if all_cnts >= num_tokens:\n",
    "                    break    \n",
    "    print(f\"Finished writing {all_cnts} tokens to {tgt} from {src}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing doc ids to /data/xzhao/experiments/roman-pretrain/exp-datasets/test/doc_ids.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xzhao/dataset/roman-pretrain/instructions/ja/medical_native.jsonl → /data/xzhao/experiments/roman-pretrain/exp-datasets/test/knolwedge.jsonl (5.00e+08 tokens): 3461201it [02:51, 20178.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing 500000571 tokens to /data/xzhao/experiments/roman-pretrain/exp-datasets/test/knolwedge.jsonl from /data/xzhao/dataset/roman-pretrain/instructions/ja/medical_native.jsonl\n",
      "Writing doc ids to /data/xzhao/experiments/roman-pretrain/exp-datasets/test/doc_ids.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xzhao/dataset/roman-pretrain/instructions/en/native.subset.jsonl → /data/xzhao/experiments/roman-pretrain/exp-datasets/test/knolwedge.jsonl (5.00e+08 tokens): 2050597it [02:39, 12831.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing 500000264 tokens to /data/xzhao/experiments/roman-pretrain/exp-datasets/test/knolwedge.jsonl from /data/xzhao/dataset/roman-pretrain/instructions/en/native.subset.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kg_fn = os.path.join(save_dir, \"knolwedge.jsonl\")\n",
    "if btoken_kg[\"ja-medical\"] > 0:\n",
    "    native_fn = os.path.join(data_root, \"ja\", \"medical_native.jsonl\")\n",
    "    write_text(native_fn, kg_fn, b_tokens=btoken_kg[\"ja-medical\"], tokenizer_type=tokenizer_type, lang='ja', record_ids=True, docids=None)\n",
    "if btoken_kg[\"en-medical\"] > 0:\n",
    "    native_fn = os.path.join(data_root, \"en\", \"native.subset.jsonl\")\n",
    "    write_text(native_fn, kg_fn, b_tokens=btoken_kg[\"en-medical\"], tokenizer_type=tokenizer_type, lang='en', record_ids=True, docids=None)\n",
    "if btoken_kg[\"zh-medical\"] > 0:\n",
    "    native_fn = os.path.join(data_root, \"zh\", \"medical_native.jsonl\")\n",
    "    write_text(native_fn, kg_fn, b_tokens=btoken_kg[\"zh-medical\"], tokenizer_type=tokenizer_type, lang='zh', record_ids=True, docids=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xzhao/dataset/roman-pretrain/instructions/ja/balanced_trans.jsonl → /data/xzhao/experiments/roman-pretrain/exp-datasets/test/transfer.jsonl (1.00e+09 tokens): 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xzhao/dataset/roman-pretrain/instructions/ja/balanced_trans.jsonl → /data/xzhao/experiments/roman-pretrain/exp-datasets/test/transfer.jsonl (1.00e+09 tokens): 16967008it [06:45, 41841.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing 1000000020 tokens to /data/xzhao/experiments/roman-pretrain/exp-datasets/test/transfer.jsonl from /data/xzhao/dataset/roman-pretrain/instructions/ja/balanced_trans.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ct_fn = os.path.join(save_dir, \"transfer.jsonl\")\n",
    "\n",
    "docid_fn = f\"{save_dir}/doc_ids.jsonl\"\n",
    "lang2docid = {}\n",
    "for item in load_json(docid_fn):\n",
    "    if item['lang'] not in lang2docid:\n",
    "        lang2docid[item['lang']] = set()\n",
    "    lang2docid[item['lang']].add(item['docid'])\n",
    "    \n",
    "for lang_pair in collections['crosslingual-transfer']:\n",
    "    btoken_ct = collections['crosslingual-transfer'][lang_pair]\n",
    "    for data_type in btoken_ct:\n",
    "        if data_type == \"zh-ja\":\n",
    "            raise NotImplementedError('zh-ja is not supported currently')\n",
    "        if btoken_ct == 0 or btoken_ct[data_type] == 0:\n",
    "            continue\n",
    "        \n",
    "        assert lang_pair.startswith(\"en-\")\n",
    "        \n",
    "        data_dir = os.path.join(data_root, lang_pair[3:])\n",
    "        filename = os.path.join(data_root, lang_pair[3:], f\"{data_type.replace('-', '_')}.jsonl\")\n",
    "        if not os.path.exists(filename):\n",
    "            raise FileNotFoundError(f\"File not found: {filename}\")\n",
    "        \n",
    "        if not data_type.startswith(\"medical-\"):\n",
    "            write_text(filename, ct_fn, b_tokens=btoken_ct[data_type], tokenizer_type=tokenizer_type)\n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
