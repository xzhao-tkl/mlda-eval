{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm\n",
    "from utils import DATA_ROOT, load_jsonl, dump_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1083830/1083830 [00:11<00:00, 91542.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared the raw Japanese J-stage dataset with 614444 to /data/xzhao/dataset/roman-pretrain/ja/data.jsonl.\n",
      "Prepared the raw English J-stage dataset with 614444 to /data/xzhao/dataset/roman-pretrain/en_pair/data.jsonl.\n"
     ]
    }
   ],
   "source": [
    "## Prepare J-stage raw dataset for ja and en(-pair)\n",
    "def create_jstage_dataset():\n",
    "    src = \"/data/xzhao/dataset/meta_med/clean/metadata_med_ddp.jsonl\"\n",
    "\n",
    "    ja_tgt_dir = os.path.join(DATA_ROOT, \"datasets\", \"medical\", \"ja\")\n",
    "    en_tgt_dir = os.path.join(DATA_ROOT, \"datasets\", \"medical\", \"en_pair\")\n",
    "\n",
    "    os.makedirs(ja_tgt_dir, exist_ok=True)\n",
    "    os.makedirs(en_tgt_dir, exist_ok=True)\n",
    "    ja_tgt = os.path.join(ja_tgt_dir, 'data.jsonl')\n",
    "    en_tgt = os.path.join(en_tgt_dir, 'data.jsonl')\n",
    "\n",
    "    if os.path.exists(ja_tgt) and os.path.exists(en_tgt): \n",
    "        print(f\"Skip: {ja_tgt} and {en_tgt} is created.\")\n",
    "        return\n",
    "\n",
    "    items = load_jsonl(src)\n",
    "    allowed_subjects = [\"医学\", \"薬学\", \"歯学\"]\n",
    "\n",
    "    en_new_items, ja_new_items = [], []\n",
    "    for item in tqdm(items):\n",
    "        subjects, keywords = [], []\n",
    "        doc_id = item['articleId']['journalCode'] + \"@@\" + item['articleId']['articleCode']\n",
    "        for data in item['title']:\n",
    "            if data['language'] == \"en\":\n",
    "                en_doc_tit = data['content']\n",
    "            else:\n",
    "                ja_doc_tit = data['content']\n",
    "        for data in item['abstract']:\n",
    "            if data['language'] == \"en\":\n",
    "                en_doc_abs = data['content']\n",
    "            else:\n",
    "                ja_doc_abs = data['content']\n",
    "        \n",
    "        record = False\n",
    "        if 'subjects' in item['journalInfo'] and item['journalInfo']['subjects'] is not None:\n",
    "            en_subjects = [subject['content'] for subject in item['journalInfo']['subjects'] if subject['language']==\"en\"]\n",
    "            ja_subjects = [subject['content'] for subject in item['journalInfo']['subjects'] if subject['language']==\"ja\"]\n",
    "            for subject in ja_subjects:\n",
    "                record = any([asub in subject for asub in allowed_subjects])\n",
    "                if record:\n",
    "                    break\n",
    "        if not record:\n",
    "            continue\n",
    "\n",
    "        if 'keywords' in item and item['keywords'] is not None:\n",
    "            en_keywords = [keyword['content'] for keyword in item['keywords'] if keyword['language']==\"en\"]\n",
    "            ja_keywords = [keyword['content'] for keyword in item['keywords'] if keyword['language']==\"ja\"]\n",
    "        \n",
    "        en_new_items.append({\n",
    "            'docid': doc_id,\n",
    "            'title': en_doc_tit,\n",
    "            'abstract': en_doc_abs,\n",
    "            'subjects': en_subjects,\n",
    "            'keywords': en_keywords,\n",
    "        })\n",
    "\n",
    "        ja_new_items.append({\n",
    "            'docid': doc_id,\n",
    "            'title': ja_doc_tit,\n",
    "            'abstract': ja_doc_abs,\n",
    "            'subjects': ja_subjects,\n",
    "            'keywords': ja_keywords,\n",
    "        })\n",
    "        \n",
    "    dump_jsonl(ja_new_items, ja_tgt)\n",
    "    print(f\"Prepared the raw Japanese J-stage dataset with {len(ja_new_items)} to {ja_tgt}.\")\n",
    "    dump_jsonl(en_new_items, en_tgt)\n",
    "    print(f\"Prepared the raw English J-stage dataset with {len(en_new_items)} to {en_tgt}.\")\n",
    "\n",
    "create_jstage_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare English Pubmed dataset\n",
    "### Need to follow the instructions in https://github.com/thoppe/The-Pile-PubMed to download and process the data\n",
    "### NOTE: revise p2 to define our format; do not need run p3\n",
    "### The code and data is saved at /model/data-scidoc/roman-pretrain/datasets/Pile\n",
    "\n",
    "tgt_fn = os.path.join(DATA_ROOT, 'en', \"data.jsonl\")\n",
    "assert os.path.exists(tgt_fn)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare Chinese medical domain dataset\n",
    "\n",
    "tgt_fn = os.path.join(DATA_ROOT, \"datasets\", \"medical\", \"zh\", \"data.jsonl\")\n",
    "\n",
    "if os.path.exists(tgt_fn):\n",
    "    sys.exit(0)\n",
    "\n",
    "tgt_dir = os.path.join(DATA_ROOT, 'zh')\n",
    "os.makedirs(tgt_dir, exist_ok=True)\n",
    "src_fn = os.path.join(DATA_ROOT, 'datasets', 'raw', 'csl_camera_readly.tsv')\n",
    "\n",
    "items = []\n",
    "with open(src_fn, \"r\", encoding=\"utf8\") as fn:\n",
    "    for i, line in enumerate(fn):\n",
    "        fields = line.strip().split(\"\\t\")\n",
    "        subjects = [fields[3], fields[4]]\n",
    "        if \"医学\" in subjects or \"药学\" in subjects or \"医药\" in subjects:\n",
    "            items.append({\n",
    "                \"docid\": i, \n",
    "                \"title\": fields[0],\n",
    "                \"abstract\": fields[1],\n",
    "                \"keywords\": fields[2].split(\"_\"),\n",
    "                \"subjects\": subjects,\n",
    "            })\n",
    "\n",
    "dump_jsonl(items, tgt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare for English-Japanese balanced bilingual corpora\n",
    "\n",
    "ROOT = os.path.join(DATA_ROOT, \"datasets\", \"balanced_bilingual\", \"en-ja\")\n",
    "\n",
    "infn = os.path.join(ROOT, \"en-ja\", \"en-ja.bicleaner05.txt\")\n",
    "outfn = os.path.join(ROOT, \"data.jsonl\")\n",
    "outfp = open(outfn, 'w', encoding=\"utf8\")\n",
    "\n",
    "with open(infn, 'r', encoding=\"utf8\") as fp:\n",
    "    for line in tqdm(fp):\n",
    "        _, _, score, en_text, ja_text = line.split(\"\\t\")\n",
    "        score = float(score)\n",
    "        if score < 0.7:\n",
    "            continue\n",
    "        \n",
    "        item = {\n",
    "            \"en\": en_text.strip(),\n",
    "            \"ja\": ja_text.strip(),\n",
    "            \"score\": score\n",
    "        }\n",
    "        string = json.dumps(item, ensure_ascii=False)\n",
    "        outfp.write(f\"{string}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare for Chinese-Japanese balanced bilingual corpora\n",
    "\n",
    "ROOT = os.path.join(DATA_ROOT, \"datasets\", \"balanced_bilingual\", \"zh-ja\")\n",
    "\n",
    "for filename in [\"zh-ja.bicleaner05.txt\", \"zh-ja.crowdsourcing_b05l07.txt\"]:\n",
    "    infn = os.path.join(ROOT, \"zh-ja\", filename)\n",
    "    outfn = os.path.join(ROOT, \"data.jsonl\")\n",
    "    outfp = open(outfn, 'a', encoding=\"utf8\")\n",
    "\n",
    "    with open(infn, 'r', encoding=\"utf8\") as fp:\n",
    "        for line in tqdm(fp):\n",
    "            if filename == \"zh-ja.bicleaner05.txt\":\n",
    "                _, score, zh_text, ja_text = line.split(\"\\t\")\n",
    "            else:\n",
    "                _, _, score, zh_text, ja_text = line.split(\"\\t\")\n",
    "            score = float(score)\n",
    "            if score < 0.7:\n",
    "                continue\n",
    "            \n",
    "            item = {\n",
    "                \"zh\": zh_text.strip(),\n",
    "                \"ja\": ja_text.strip(),\n",
    "                \"score\": score\n",
    "            }\n",
    "            string = json.dumps(item, ensure_ascii=False)\n",
    "            outfp.write(f\"{string}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare for Chinese-English balanced bilingual corpora\n",
    "\n",
    "ROOT = os.path.join(DATA_ROOT, \"datasets\", \"balanced_bilingual\", \"en-zh\")\n",
    "\n",
    "indir = os.path.join(ROOT, \"UM-Corpus/data/Bilingual\")\n",
    "outfn = os.path.join(ROOT, \"data.jsonl\")\n",
    "outfp = open(outfn, 'a', encoding=\"utf8\")\n",
    "\n",
    "items = []\n",
    "for folder in os.listdir(indir):\n",
    "    fns = os.listdir(os.path.join(indir, folder))\n",
    "    assert len(fns) == 1\n",
    "    infn = os.path.join(indir, folder, fns[0])\n",
    "\n",
    "    with open(infn, 'r', encoding=\"utf8\") as fp:    \n",
    "        for i, line in tqdm(enumerate(fp)):\n",
    "            if i % 2 == 0:\n",
    "                items.append({\n",
    "                    \"en\": line.strip()})\n",
    "            else:\n",
    "                items[-1]['zh'] = line.strip()\n",
    "        \n",
    "    for item in items:\n",
    "        string = json.dumps(item, ensure_ascii=False)\n",
    "        outfp.write(f\"{string}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare for scientific Chinese-English balanced bilingual corpora\n",
    "\n",
    "ROOT = os.path.join(DATA_ROOT, \"datasets\", \"scientific_bilingual\", \"en-zh\")\n",
    "tgt_fn = os.path.join(ROOT, \"zh-only.jsonl\")\n",
    "\n",
    "if os.path.exists(tgt_fn):\n",
    "    sys.exit(0)\n",
    "\n",
    "tgt_dir = os.path.join(DATA_ROOT, 'zh')\n",
    "os.makedirs(tgt_dir, exist_ok=True)\n",
    "src_fn = os.path.join(DATA_ROOT, 'datasets', 'raw', 'csl_camera_readly.tsv')\n",
    "\n",
    "items = []\n",
    "with open(src_fn, \"r\", encoding=\"utf8\") as fn:\n",
    "    for i, line in enumerate(fn):\n",
    "        fields = line.strip().split(\"\\t\")\n",
    "        subjects = [fields[3], fields[4]]\n",
    "        if \"医学\" not in subjects and \"药学\" not in subjects and \"医药\" not in subjects:\n",
    "            items.append({\n",
    "                \"docid\": i, \n",
    "                \"title\": fields[0],\n",
    "                \"abstract\": fields[1],\n",
    "                \"keywords\": fields[2].split(\"_\"),\n",
    "                \"subjects\": subjects,\n",
    "            })\n",
    "\n",
    "dump_jsonl(items, tgt_fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1790it [00:00, 89354.15it/s]\n",
      "1790it [00:00, 89354.15it/s]\n",
      "1784it [00:00, 89332.14it/s]\n",
      "1812it [00:00, 90818.78it/s]\n",
      "1000000it [00:08, 122964.64it/s]\n",
      "1000000it [00:07, 126659.38it/s]\n",
      "1008500it [00:07, 137539.66it/s]\n"
     ]
    }
   ],
   "source": [
    "## Prepare for scientific English-Japanese balanced bilingual corpora\n",
    "import re\n",
    "ROOT = os.path.join(DATA_ROOT, \"datasets\", \"scientific_bilingual\", \"en-ja\")\n",
    "tgt_fn = os.path.join(ROOT, \"data.jsonl\")\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "\n",
    "if os.path.exists(tgt_fn):\n",
    "    sys.exit(0)\n",
    "\n",
    "\n",
    "outfp = open(tgt_fn, 'w', encoding=\"utf8\")\n",
    "indir = os.path.join(DATA_ROOT, \"datasets\", \"raw/ASPEC/ASPEC-JE\")\n",
    "\n",
    "docids = set()\n",
    "for filename in [\"dev/dev.txt\", \"devtest/devtest.txt\", \"test/test.txt\", \"train/train-1.txt\", \"train/train-2.txt\", \"train/train-3.txt\"]:\n",
    "    infn = os.path.join(indir, filename)\n",
    "    with open(infn, 'r', encoding=\"utf8\") as fp:\n",
    "        for line in tqdm(fp):\n",
    "            if 'train' not in filename:\n",
    "                docid, index, ja_text, en_text = line.split(\"|||\")\n",
    "                docid = f'{docid.strip()}-{index.strip()}'\n",
    "            else:\n",
    "                score, docid, index, ja_text, en_text = line.split(\"|||\")\n",
    "                docid = f'{docid}-{index}'\n",
    "            \n",
    "            assert re.match(r\"^\\s*[A-Z]-.*\", docid), docid\n",
    "            if re.match(r\"^\\s*[CEGXY]-.*\", docid):\n",
    "                continue\n",
    "\n",
    "            assert docid not in docids\n",
    "            docids.add(docid)\n",
    "            item = {\n",
    "                \"docid\": docid,\n",
    "                \"ja\": ja_text.strip(),\n",
    "                \"en\": en_text.strip(),\n",
    "            }\n",
    "            string = json.dumps(item, ensure_ascii=False)\n",
    "            outfp.write(f\"{string}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2090it [00:00, 86970.41it/s]\n",
      "2090it [00:00, 86970.41it/s]\n",
      "2148it [00:00, 85994.30it/s]\n",
      "2107it [00:00, 80710.52it/s]\n",
      "672315it [00:05, 112536.16it/s]\n"
     ]
    }
   ],
   "source": [
    "## Prepare for scientific Chinese-Japanese balanced bilingual corpora\n",
    "\n",
    "ROOT = os.path.join(DATA_ROOT, \"datasets\", \"scientific_bilingual\", \"zh-ja\")\n",
    "tgt_fn = os.path.join(ROOT, \"data.jsonl\")\n",
    "\n",
    "outfp = open(tgt_fn, 'w', encoding=\"utf8\")\n",
    "indir = os.path.join(DATA_ROOT, \"datasets\", \"raw/ASPEC/ASPEC-JC\")\n",
    "\n",
    "docids = set()\n",
    "for filename in [\"dev/dev.txt\", \"devtest/devtest.txt\", \"test/test.txt\", \"train/train.txt\"]:\n",
    "    infn = os.path.join(indir, filename)\n",
    "\n",
    "    with open(infn, 'r', encoding=\"utf8\") as fp:\n",
    "        for line in tqdm(fp):\n",
    "            docid, ja_text, zh_text = line.split(\"|||\")\n",
    "\n",
    "            assert docid not in docids\n",
    "            docids.add(docid)\n",
    "            \n",
    "            item = {\n",
    "                \"docid\": docid,\n",
    "                \"ja\": ja_text.strip(),\n",
    "                \"zh\": zh_text.strip(),\n",
    "            }\n",
    "            string = json.dumps(item, ensure_ascii=False)\n",
    "            outfp.write(f\"{string}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
