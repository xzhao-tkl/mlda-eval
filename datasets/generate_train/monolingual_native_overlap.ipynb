{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbc97717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from utils import EXP_ROOT, load_jsonl_iteratively\n",
    "data_dir = os.path.join(EXP_ROOT, 'datasets/kg-datasets/ja-0.5/eval_qa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d513d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already generated docids.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Record and read document IDs for evaluation, \n",
    "# These documents are used as search basis for measuring token overlap\n",
    "from utils import dump_jsonl, load_jsonl\n",
    "\n",
    "eval_ids_fn = os.path.join(data_dir, \"docids.jsonl\")\n",
    "\n",
    "if os.path.exists(eval_ids_fn) and len(load_jsonl(eval_ids_fn)) == 10000:\n",
    "    print(\"Already generated docids.jsonl\")\n",
    "else:\n",
    "    base_docids = {}\n",
    "    for path in os.listdir(data_dir):\n",
    "        if os.path.isfile(os.path.join(data_dir, path)):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(data_dir, path, \"response.s0e10000.jsonl\")\n",
    "        base_docids[path] = set()\n",
    "        if os.path.exists(file_path):\n",
    "            for item in load_jsonl_iteratively(file_path):\n",
    "                base_docids[path].add(item['id'][9:])\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "    assert base_docids[\"trisent-prompt\"] == base_docids[\"monosent-prompt\"]\n",
    "    assert base_docids[\"trisent-prompt\"] == base_docids[\"bisent-prompt\"]\n",
    "    assert len(base_docids[\"trisent-prompt\"]) == 10000\n",
    "\n",
    "base_docids = load_jsonl(eval_ids_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e44bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xzhao/.pyenv/versions/poetry-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already generated tokens.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "614444it [00:29, 21150.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all documents\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from utils import DATA_ROOT\n",
    "\n",
    "token_fn = os.path.join(DATA_ROOT, \"datasets/medical/ja/tokens.jsonl\")\n",
    "raw_fn = os.path.join(DATA_ROOT, \"datasets/medical/ja/data.jsonl\")\n",
    "if os.path.exists(token_fn):\n",
    "    print(\"Already generated tokens.jsonl\")\n",
    "    id2tokens = {} \n",
    "    for item in tqdm(load_jsonl_iteratively(token_fn)):\n",
    "        id2tokens.update(item)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"llm-jp/llm-jp-3-7.2b\")\n",
    "    id2tokens = {}\n",
    "    for item in tqdm(load_jsonl_iteratively(raw_fn)):\n",
    "        data = item[\"abstract\"]\n",
    "        inputs = tokenizer(data, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "        id2tokens[item[\"docid\"]] = inputs.tolist()[0]\n",
    "        \n",
    "    dump_jsonl([{id: tokens} for id, tokens in id2tokens.items()], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6499bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already generated token_overlap.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "604444it [00:05, 106419.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# Measure the token overlap between the base docs and the left docs\n",
    "token_overlap_fn = os.path.join('./caches/token_overlap.jsonl')\n",
    "if os.path.exists(token_overlap_fn):\n",
    "    print(\"Already generated token_overlap.jsonl\")\n",
    "    docid2overlap = {}\n",
    "    for item in tqdm(load_jsonl_iteratively(token_overlap_fn)):\n",
    "        docid2overlap.update(item)\n",
    "else:\n",
    "    base_tokens = set()\n",
    "    for item in tqdm(load_jsonl_iteratively(raw_fn)):\n",
    "        if item[\"docid\"] in base_docids:\n",
    "            base_tokens.update(id2tokens[item[\"docid\"]])\n",
    "        \n",
    "    docid2overlap = {}\n",
    "    for docid in tqdm(id2tokens):\n",
    "        if docid in base_docids:\n",
    "            continue\n",
    "        tokens = set(id2tokens[docid])\n",
    "        overlap = len(tokens.intersection(base_tokens)) / len(tokens)\n",
    "        docid2overlap[docid] = (overlap, len(tokens))\n",
    "    dump_jsonl([{docid: (overlap, tokens)} for docid, (overlap, tokens) in docid2overlap.items()], token_overlap_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1501a34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already generated id2instructions.pkl\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Read all intructions in medical_native.jsonl to a dict by docid\n",
    "instructions_fn = os.path.join(DATA_ROOT, \"instructions/ja/medical_native.jsonl\")\n",
    "assert os.path.exists(instructions_fn), f\"File not found: {instructions_fn}\"\n",
    "\n",
    "pos_pkl = \"./caches/id2instructions.pkl\"\n",
    "\n",
    "if os.path.exists(pos_pkl):\n",
    "    print(\"Already generated id2instructions.pkl\")\n",
    "    id2instructions = pickle.load(open(pos_pkl, \"rb\"))\n",
    "else:\n",
    "    id2instructions = defaultdict(list)\n",
    "\n",
    "    with open(instructions_fn, \"r\", encoding=\"utf8\") as f:\n",
    "        pbar = tqdm(desc=\"Indexing\", unit=\" lines\")\n",
    "        while True:\n",
    "            pos = f.tell()\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            entry = json.loads(line)\n",
    "            id2instructions[entry[\"docid\"]].append(pos)\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    pickle.dump(id2instructions, open(pos_pkl, \"wb\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe8554e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1077.63it/s]\n"
     ]
    }
   ],
   "source": [
    "high_overlap_fn = os.path.join(DATA_ROOT, \"instructions/ja/medical_native_high_overlap.jsonl\")\n",
    "low_overlap_fn = os.path.join(DATA_ROOT, \"instructions/ja/medical_native_low_overlap.jsonl\")\n",
    "in_f = open(instructions_fn, \"r\", encoding=\"utf8\")\n",
    "\n",
    "with open(high_overlap_fn, \"w\", encoding=\"utf8\") as out_f1, \\\n",
    "     open(low_overlap_fn, \"w\", encoding=\"utf8\") as out_f2:\n",
    "    for docid in tqdm(base_docids):\n",
    "        assert docid in id2tokens, f\"Missing tokens for {docid}\"\n",
    "        assert len(id2instructions[docid]) != 0, f\"Missing instructions for {docid}\"\n",
    "        for pos in id2instructions[docid]:\n",
    "            in_f.seek(pos)\n",
    "            line = in_f.readline()\n",
    "            out_f1.write(line)\n",
    "            out_f2.write(line)\n",
    "    \n",
    "in_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6afec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = sorted(docid2overlap.items(), key=lambda x: x[1][0], reverse=True)\n",
    "sorted_docs = [(docid, overlap, tokens) for docid, (overlap, tokens) in docs if tokens >= 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dedb5f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lca@@16/3/16_188', 0.5513513513513514, 185),\n",
       " ('jasmin@@2009f/0/2009f_0_34', 0.5909090909090909, 88),\n",
       " ('jceeek@@2011/0/2011_105', 0.6, 65),\n",
       " ('jceeek@@2016/0/2016_268', 0.6, 80),\n",
       " ('jsmbe@@Annual56/Proc/Annual56_1', 0.6, 60),\n",
       " ('jfsc@@130/0/130_794', 0.6043956043956044, 91),\n",
       " ('jceeek@@2010/0/2010_0_83', 0.6119402985074627, 67),\n",
       " ('jfsc@@126/0/126_789', 0.6134453781512605, 119),\n",
       " ('toxp@@33/0/33_0_54', 0.6136363636363636, 176),\n",
       " ('pjsai@@JSAI2018/0/JSAI2018_2K104', 0.6190476190476191, 63)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_docs[::-1][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9943f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 587673/587673 [31:19<00:00, 312.70it/s] \n"
     ]
    }
   ],
   "source": [
    "in_f = open(instructions_fn, \"r\", encoding=\"utf8\")\n",
    "\n",
    "with open(high_overlap_fn, \"a\", encoding=\"utf8\") as out_f:\n",
    "    for docid, overlap, tokens in tqdm(sorted_docs):\n",
    "        for pos in id2instructions[docid]:\n",
    "            in_f.seek(pos)\n",
    "            line = in_f.readline()\n",
    "            out_f.write(line)\n",
    "in_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9a7c506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 587673/587673 [17:03<00:00, 574.44it/s]  \n"
     ]
    }
   ],
   "source": [
    "in_f = open(instructions_fn, \"r\", encoding=\"utf8\")\n",
    "\n",
    "with open(low_overlap_fn, \"a\", encoding=\"utf8\") as out_f:\n",
    "    for docid, overlap, tokens in tqdm(sorted_docs[::-1]):\n",
    "        for pos in id2instructions[docid]:\n",
    "            in_f.seek(pos)\n",
    "            line = in_f.readline()\n",
    "            out_f.write(line)\n",
    "in_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204375e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
