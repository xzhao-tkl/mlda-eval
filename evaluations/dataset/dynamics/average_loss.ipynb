{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede228f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"llm-jp/llm-jp-3-13b\"  # adjust to your LLM-jp variant\n",
    "batch_sequences = [\n",
    "    \"こんにちは、お元気ですか？\",\n",
    "    \"機械学習はどのように進化していますか？\"\n",
    "]\n",
    "last_ffn_layer_name = \"<last_ffn_layer_name>\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "model.eval().to(\"cuda\")\n",
    "\n",
    "# Freeze all params\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Enable gradients for FFN layer only\n",
    "last_ffn = dict(model.named_modules())[last_ffn_layer_name]\n",
    "for p in last_ffn.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "inputs = tokenizer(batch_sequences, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "labels = input_ids.clone()\n",
    "labels[:, :-1] = input_ids[:, 1:]\n",
    "labels[:, -1] = -100\n",
    "\n",
    "outputs = model(input_ids, labels=labels)\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "\n",
    "total_grad = 0.0\n",
    "param_count = 0\n",
    "for p in last_ffn.parameters():\n",
    "    if p.grad is not None:\n",
    "        total_grad += p.grad.abs().mean().item()\n",
    "        param_count += 1\n",
    "\n",
    "avg_grad = total_grad / param_count if param_count else 0.0\n",
    "print(f\"Average FFN gradient magnitude (batch): {avg_grad:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
