{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1543cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "次の文章に続きの文を加えてください。\n",
      "我々は, 細菌感染によってウサギに慢性副鼻腔炎を惹起することにより, 副鼻腔及び鼻腔にポリープが形成されるのを認めた。...\n",
      "これらは組織学的特徴から,「肉芽性ポリープ」および「浮腫性ポリープ」の二つの型に分けられた。 「肉芽性ポリープ」は上皮を完全に失った部で, 線維芽細胞の増殖と血管新生が起こって肉芽が洞内に突出することによって形成され, それを被うように未分化な上皮細胞が増殖移動していくように観察された。 「浮腫性ポリープ」は, 上皮細胞が脱落したり, 成熟した形態を失って背の低い未分化な形態を示したり, 扁平な上皮細胞が重層して観察されたりする部で, 形成されていた。 本研究の結果は, 上皮細胞も鼻茸形成の初期において, なんらかの役割を果たしていることを示唆する。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from utils import load_jsonl_iteratively \n",
    "\n",
    "lang = \"ja\"\n",
    "train_data_root = \"/data/xzhao/experiments/med-eval/dataset/losses/train_samples\"\n",
    "data_path = os.path.join(train_data_root, f\"random_{lang}_2k.jsonl\")\n",
    "\n",
    "for item in load_jsonl_iteratively(data_path):\n",
    "    print(item[\"text\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3015353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"resources/wnjpn.db\")\n",
    "def csv_input(path_name):\n",
    "    rows = []\n",
    "    with open(path_name,encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def SearchSimilarWords(word, mode=\"simple\"):\n",
    "    assert mode in [\"simple\", \"detailed\"], \"mode must be 'simple' or 'detailed'.\"\n",
    "    cur = conn.execute(\"select wordid from word where lemma='%s'\" % word)\n",
    "    word_id = 99999999 \n",
    "    for row in cur:\n",
    "        word_id = row[0]\n",
    "\n",
    "    if word_id==99999999:\n",
    "        return\n",
    "    cur = conn.execute(\"select synset from sense where wordid='%s'\" % word_id)\n",
    "    synsets = []\n",
    "    for row in cur:\n",
    "        synsets.append(row[0])\n",
    "    simdict = {}\n",
    "\n",
    "    en_syns, ja_syns = [], []\n",
    "    for synset in synsets:\n",
    "        # cur1 = conn.execute(\"select name from synset where synset='%s'\" % synset)\n",
    "        cur2 = conn.execute(\"select def from synset_def where (synset='%s' and lang='jpn')\" % synset)\n",
    "        cur3 = conn.execute(\"select wordid from sense where (synset='%s' and wordid!=%s)\" % (synset,word_id))\n",
    "            \n",
    "        if mode == \"simple\":\n",
    "            for row3 in cur3:\n",
    "                target_word_id = row3[0]\n",
    "                cur3_1 = conn.execute(\"select lemma, lang from word where wordid=%s\" % target_word_id)\n",
    "                for row3_1 in cur3_1:\n",
    "                    if row3_1[1] == \"jpn\":\n",
    "                        ja_syns.append(row3_1[0])\n",
    "                    elif row3_1[1] == \"eng\":\n",
    "                        if \"_\" in row3_1[0]:\n",
    "                            en_syns.append(row3_1[0].replace(\"_\", \" \"))\n",
    "                        else:\n",
    "                            en_syns.append(row3_1[0])\n",
    "                    else:\n",
    "                        raise ValueError(\"Unknown language: %s\" % row3_1[1])\n",
    "        elif mode == \"detailed\":\n",
    "            simdict[synset] = {\"definition\": \"\", \"words\": []}\n",
    "            simdict[synset][\"definition\"] = \". \".join([row[0] for row in cur2])\n",
    "            for row3 in cur3:\n",
    "                target_word_id = row3[0]\n",
    "                cur3_1 = conn.execute(\"select lemma, lang from word where wordid=%s\" % target_word_id)\n",
    "                for row3_1 in cur3_1:\n",
    "                    simdict[synset][\"words\"].append(row3_1)\n",
    "\n",
    "    if mode == \"simple\":\n",
    "        simdict[\"ja\"] = random.choice(ja_syns) if ja_syns else None\n",
    "        simdict[\"en\"] = random.choice(en_syns) if en_syns else None\n",
    "        if simdict[\"ja\"] == None and simdict[\"en\"] == None:\n",
    "            return None\n",
    "    return simdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "395a371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3882146/2287453364.py:10: DeprecationWarning: Parameter dict_type of Dictionary() is deprecated, use dict instead\n",
      "  tokenizer_obj = dictionary.Dictionary(dict_type=\"full\").create()\n"
     ]
    }
   ],
   "source": [
    "# Mecabなど形態素解析で使うIPA品詞体系 https://miner.hatenablog.com/entry/323\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "import stopwordsiso as stopwords\n",
    "\n",
    "japanese_stopwords = stopwords.stopwords(\"ja\")\n",
    "english_stop_words = stopwords.stopwords(\"en\")\n",
    "stopwords = japanese_stopwords.union(english_stop_words)\n",
    "\n",
    "tokenizer_obj = dictionary.Dictionary(dict_type=\"full\").create()\n",
    "mode = tokenizer.Tokenizer.SplitMode.C\n",
    "\n",
    "def process_syn(text):\n",
    "    text = item[\"text\"]\n",
    "    tokens = tokenizer_obj.tokenize(text, mode)\n",
    "\n",
    "    words = [(m.surface(), m.part_of_speech(), (m.begin(), m.end())) for m in tokens]\n",
    "    syns = []\n",
    "    \n",
    "    def get_word_syn(word, pos, index):\n",
    "        try:\n",
    "            syns = SearchSimilarWords(word)\n",
    "        except Exception as e:\n",
    "            return None\n",
    "        if not syns:\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            \"surface\": word,\n",
    "            \"pos\": pos,\n",
    "            \"index\": index,\n",
    "            \"synonyms\": syns if syns else []\n",
    "        }\n",
    "\n",
    "    for word in words:\n",
    "        if len(word[0]) < 2 or word[0] in stopwords:\n",
    "            continue\n",
    "        \n",
    "        if (word[1][0] == \"名詞\" and word[1][1] in [\"普通名詞\", \"固有名詞\", \"サ変接続\", \"形容動詞語幹\"]) or \\\n",
    "            (word[1][0] == \"動詞\" and word[1][1] == \"一般\"):\n",
    "            syn_item = get_word_syn(word[0], word[1], word[2])\n",
    "            if syn_item:\n",
    "                syns.append(syn_item)\n",
    "    \n",
    "    all_words = [m.surface() for m in tokens]\n",
    "    # print(len(syns) / len(all_words))\n",
    "    return {\"syn_words\": syns, \"all_words\": all_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c220f5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:02,  4.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from utils import dump_jsonl\n",
    "\n",
    "item_with_syns = []\n",
    "for item in tqdm(load_jsonl_iteratively(data_path, request_num=10)):\n",
    "    item['wordnet_synonyms'] = process_syn(item[\"text\"])\n",
    "    item_with_syns.append(item)\n",
    "\n",
    "# dump_path = os.path.join(train_data_root, f\"random_{lang}_2k.withsyn.jsonl\")\n",
    "# dump_jsonl(item_with_syns, dump_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5299c082",
   "metadata": {},
   "source": [
    "TEST from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19efa93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xzhao/.pyenv/versions/poetry-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\"/data/xzhao/experiments/roman-pretrain/exps/exp1-multi-ja/hf_models/iter_0000010\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dba07e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein distance: 8\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "seq1 = [\"人間\", \"の\", \"体\", \"は\", \"約\", \"37兆個\", \"の\", \"細胞\", \"で\", \"構成\", \"さ\", \"れ\", \"て\", \"いる\"]\n",
    "seq2 = [\"人間\", \"の\", \"体\", \"は\", \"約\", \"37兆\", \"の\", \"DNA細胞\", \"から\", \"作ら\", \"れて\", \"いる\", \"x\"]\n",
    "\n",
    "distance = edit_distance(seq1, seq2)\n",
    "print(\"Levenshtein distance:\", distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1b435434",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5\n",
    "text = item_with_syns[idx][\"text\"]\n",
    "synonyms = item_with_syns[idx][\"wordnet_synonyms\"][\"syn_words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4023ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonym_text(synonyms, tgt_lang, prob):\n",
    "    replaced_syns = []\n",
    "    noisy_text = text\n",
    "    synonyms = [syn for syn in synonyms if syn[\"synonyms\"][tgt_lang] is not None]\n",
    "    for syn in synonyms:\n",
    "        if random.random() < prob: \n",
    "            replaced_syns.append(syn)\n",
    "\n",
    "    replaced_indexes = []\n",
    "    offset = 0\n",
    "    for synonym in replaced_syns:\n",
    "        replaced_indexes.append((synonym[\"index\"][0] + offset, synonym[\"index\"][0] + len(synonym[\"synonyms\"][tgt_lang]) + offset))\n",
    "        offset += len(synonym[\"synonyms\"][tgt_lang]) - (synonym[\"index\"][1] - synonym[\"index\"][0])\n",
    "\n",
    "    replaced_syns = sorted(replaced_syns, key=lambda x: x[\"index\"][0], reverse=True)\n",
    "    for synonym in replaced_syns:\n",
    "        noisy_text = noisy_text[:synonym[\"index\"][0]] + synonym[\"synonyms\"][tgt_lang] + noisy_text[synonym[\"index\"][1]:]\n",
    "    return noisy_text, replaced_syns[::-1], replaced_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "51eedf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced '提供' by '供与', '供与' at index 6-8\n",
      "Replaced '使用' by '使う', '使う' at index 72-74\n",
      "Replaced '場合' by '時点', '時点' at index 77-79\n",
      "Replaced '肺がん' by '肺癌', '肺癌' at index 119-121\n",
      "Replaced '治療' by '和らげる', '和らげる' at index 122-126\n",
      "Replaced 'がん' by '癌腫', '癌腫' at index 152-154\n",
      "Replaced '防ぎ' by '防護具', '防護具' at index 160-163\n"
     ]
    }
   ],
   "source": [
    "noisy_text, replaced_syns, replaced_indexes = get_synonym_text(synonyms, \"ja\", 0.5)\n",
    "for syn, index in zip(replaced_syns, replaced_indexes):\n",
    "    print(f\"Replaced '{syn['surface']}' by '{syn['synonyms']['ja']}', '{noisy_text[index[0]:index[1]]}' at index {index[0]}-{index[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b4e1db2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "適切な答えを提供してください。パーキチン（paclitaxel）とカルボプラチン（carboplatin）の組み合わせは、どのようながんの治療に使用される場合がありますか？\n",
      "パーキチンとカルボプラチンの組み合わせは、特に卵巣がん、乳がん、肺がんの治療に用いられます。これらの薬はしばしばともに使われて、がん細胞の分裂を防ぎ、それに適度なストレスをかけて治療効果を高めます。\n",
      "----------\n",
      "適切な答えを供与してください。パーキチン（paclitaxel）とカルボプラチン（carboplatin）の組み合わせは、どのような癌の緩和に使用される場合がありますか？\n",
      "パーキチンとカルボプラチンの組み合わせは、特に卵巣がん、乳がん、肺癌の治療に用いられます。これらの薬はしばしばともに使われて、癌腫細胞の分かれるを防護具、それに適度なストレスをかけて治療効果を高めます。\n"
     ]
    }
   ],
   "source": [
    "noisy_text, replaced_syns, replaced_indexes = get_synonym_text(synonyms, \"ja\", 0.5)\n",
    "tokens = llm_tokenizer(text)[\"input_ids\"]\n",
    "noisy_tokens = llm_tokenizer(noisy_text, return_offsets_mapping=True)\n",
    "noisy_token_positions = noisy_tokens[\"offset_mapping\"]\n",
    "noisy_tokens = noisy_tokens[\"input_ids\"]\n",
    "print(text)\n",
    "print(\"----------\")\n",
    "print(noisy_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "279818bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_tokens = llm_tokenizer(noisy_text, return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b2f58ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(54, 58), (71, 73), (100, 104), (151, 153), (154, 158)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaced_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4d53b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_labels = []\n",
    "for idx, (start, end) in enumerate(noisy_token_positions):\n",
    "    for index in replaced_indexes:\n",
    "        if start >= index[0] and end <= index[1]:\n",
    "            token_labels.append(\"<synonym>\")\n",
    "            break\n",
    "    if len(token_labels) <= idx:\n",
    "        token_labels.append(\"<token>\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73496deb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
